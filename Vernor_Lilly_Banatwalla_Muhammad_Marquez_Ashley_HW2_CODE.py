# -*- coding: utf-8 -*-
"""HW2_6373.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sLJ9KkxbteKjDNS47xM_-PBpSAqb9t7s

**Start:Downloading required packages**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import io
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import losses, optimizers
from tensorflow.keras import callbacks
from tensorflow.keras.models import load_model
from google.colab import files

"""**End : Downloading required Packages**

**Start: Import Data**
"""

uploaded=files.upload()

"""**End: Import Data**

**Start: Reading the data and splitting into test and train**
"""

CENTURY = pd.read_csv('CENTURY.csv')
EBRIMA = pd.read_csv('EBRIMA.csv')
GILL = pd.read_csv('GILL.csv')
###Remove Unwanted Columns###
CENTURY = CENTURY.drop(columns= ['fontVariant','m_label','orientation','m_top','m_left','originalH','originalW','h','w'])
EBRIMA = EBRIMA.drop(columns= ['fontVariant','m_label','orientation','m_top','m_left','originalH','originalW','h','w'])
GILL = GILL.drop(columns= ['fontVariant','m_label','orientation','m_top','m_left','originalH','originalW','h','w'])
#Three classes of images of "normal" characters
CL1 = CENTURY[(CENTURY.strength == .4) & (CENTURY.italic == 0)]
CL2 = EBRIMA[(EBRIMA.strength == .4) & (EBRIMA.italic == 0)]
CL3 = GILL[(GILL.strength == .4) & (GILL.italic == 0)]

#Train Test split for CL1
CL1_train, CL1_test = train_test_split(CL1,test_size=0.2,random_state=101)
#Train Test split for CL2
CL2_train, CL2_test = train_test_split(CL2,test_size=0.2,random_state=101)
#Train Test split for CL3
CL3_train, CL3_test = train_test_split(CL3,test_size=0.2,random_state=101)

#Full training set and test set
X_train = pd.concat([CL1_train,CL2_train,CL3_train])
X_test = pd.concat([CL1_test,CL2_test,CL3_test])
#need to remove font, strength, and italic column
Y_train = X_train['font']
Y_test = X_test['font']
#need to remove font, strength, and italic column
X_train = X_train.drop(columns= ['font','strength','italic'])
X_test = X_test.drop(columns = ['font','strength','italic'])
X = pd.concat([X_train,X_test])
CL1 = CL1.drop(columns = ['font','strength','italic'])
CL2 = CL2.drop(columns = ['font','strength','italic'])
CL3 = CL3.drop(columns = ['font','strength','italic'])
#convert Y_train and Y_test to categorical vectors
Y_train = pd.Categorical(pd.factorize(Y_train)[0])
Y_train = pd.DataFrame(Y_train)
Y_test = pd.Categorical(pd.factorize(Y_test)[0])
Y_test = pd.DataFrame(Y_test)
num_classes = 3
Y_train = tf.keras.utils.to_categorical(Y_train, num_classes)
Y_test = tf.keras.utils.to_categorical(Y_test, num_classes)
print(X_test.shape)
print(Y_test.shape)

"""**End: Reading The data and splitting into training and testing set**

**Start: Normalizing the Data**
"""

scaler = StandardScaler()
# Fit on training set only.
scaler.fit(X_train)
# Apply transform to both the training set and the test set.
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""**End: Normalizing the Data**

**Start :PCA**
"""

#Playing around with different code to do PCA
#this is also for h0
import sklearn as sl
from sklearn.preprocessing import StandardScaler as ss
from sklearn.decomposition import PCA 

st = ss().fit_transform(X)
pca = PCA(0.95)
pc = pca.fit_transform(st) # << to retain the components in an object
#pca.explained_variance_ratio_
print ( "Components = ", pca.n_components_ , ";\nTotal explained variance = ",
      round(pca.explained_variance_ratio_.sum(),5)  )

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.savefig('pcah0.png')

#PCA to find h0
pca = PCA(n_components=104)
pca.fit_transform(X)
pca.explained_variance_ratio_
pca.explained_variance_ratio_.cumsum()

np.cumsum(pca.explained_variance_ratio_) #102

pca = PCA().fit(X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.savefig('pca.png')

#for h1
st = ss().fit_transform(CL1)
pcah1 = PCA(0.99)
pc = pcah1.fit_transform(st) # << to retain the components in an object
#pca.explained_variance_ratio_
print ( "Components = ", pcah1.n_components_ , ";\nTotal explained variance = ",
      round(pcah1.explained_variance_ratio_.sum(),5)  )
#203

#for h2
st = ss().fit_transform(CL2)
pcah2 = PCA(0.99)
pc = pcah2.fit_transform(st) # << to retain the components in an object
#pca.explained_variance_ratio_
print ( "Components = ", pcah2.n_components_ , ";\nTotal explained variance = ",
      round(pcah2.explained_variance_ratio_.sum(),5)  )
#212

#for h3
st = ss().fit_transform(CL3)
pcah3 = PCA(0.99)
pc = pcah3.fit_transform(st) # << to retain the components in an object
#pca.explained_variance_ratio_
print ( "Components = ", pcah3.n_components_ , ";\nTotal explained variance = ",
      round(pcah3.explained_variance_ratio_.sum(),5)  )
#192

plt.plot(np.cumsum(pcah1.explained_variance_ratio_), label = 'CL1')
plt.plot(np.cumsum(pcah2.explained_variance_ratio_), label = 'CL2')
plt.plot(np.cumsum(pcah3.explained_variance_ratio_), label = 'CL3')
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.legend()
plt.savefig('pcahstar.png')

"""#h* = 607"""

# should be (X,400) 
np.array(X_train).shape

"""**END PCA**

**Start: Constucting the model with h0=103**
"""

# model for h0=103
model=Sequential()
model.add(Dense(103, input_shape = (400,), activation='relu'))
model.add(Dense(3, activation = 'softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

from tensorflow.keras import callbacks
# monitor `val_accuracy`
checkpoint=callbacks.ModelCheckpoint(filepath='BestModel.h0',monitor='val_accuracy',save_best_only=True)

Monitor = model.fit(X_train,Y_train,epochs=150,batch_size=25,validation_data=(X_test, Y_test),callbacks = [checkpoint])

model.evaluate(X_train,Y_train)

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(150)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,150])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

val_loss.index(min(val_loss[10:20])) #14

"""**Start computation of confusion matrices**
 
"""

# Performance matrix test set
Y_Pred=model.predict_classes(X_test)

Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0]
  else:
    new_list=[0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

# Performance matrix training set
Y_Pred=model.predict_classes(X_train)

#Converting from lable encoding to one hot encoding
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0]
  else:
    new_list=[0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

#converting list to array
Y_Pred_array=np.array(Y_Pred_hot)

#conf matrix
from sklearn.metrics import confusion_matrix

confusion_matrix(Y_train.argmax(axis=1), Y_Pred_array.argmax(axis=1))

X_test.shape

"""**End computation of confusion  matrices**

**Start: Constucting the model with h*=607**
"""

# model for h*=607
model=Sequential()
model.add(Dense(607, input_shape = (400,), activation='relu'))
model.add(Dense(3, activation = 'softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

from tensorflow.keras import callbacks
# monitor `val_accuracy`
checkpoint=callbacks.ModelCheckpoint(filepath='BestModel.h607',monitor='val_accuracy',save_best_only=True)

Monitor = model.fit(X_train,Y_train,epochs=150,batch_size=25,validation_data=(X_test, Y_test),callbacks = [checkpoint])

model.evaluate(X_train,Y_train)

"""Sparsity Training for h=607 at Sparsity Level 10% and 20%

"""

class SparsityRegularizationLayer(layers.Layer):
  C = 0.01
  Tar = 0.1
  def call(self, inputs):
    avAL = tf.reduce_mean(inputs)
    self.add_loss(  self.C * (  -1*self.Tar*tf.math.log(avAL) + (self.Tar-1)*tf.math.log(1-avAL)  )  )
    return inputs

inputs = keras.Input(shape=(400,))
h_star = 607
x1 = layers.Dense(h_star,activation="sigmoid")(inputs)
x1 = SparsityRegularizationLayer()(x1)
outputs = layers.Dense(3, activation="softmax")(x1)
model = keras.Model(inputs=inputs,outputs=outputs)
optimizer = keras.optimizers.Adam()
loss_fn = keras.losses.SparseCategoricalCrossentropy()

model.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['accuracy'])
from tensorflow.keras import callbacks
# monitor `val_accuracy`
checkpoint=callbacks.ModelCheckpoint(filepath='BestModel.10',monitor='val_accuracy',save_best_only=True)


Monitor = model.fit(X_train,Y_train,epochs=150, batch_size=25,validation_data=(X_test,Y_test),callbacks=[checkpoint])

model.evaluate(X_train,Y_train)

X_test.shape

# Performance matrix training set
Y_Pred=model.predict(X_train)

#converting list to array
Y_Pred_array=np.array(Y_Pred)

#conf matrix
from sklearn.metrics import confusion_matrix

confusion_matrix(Y_train.argmax(axis=1), Y_Pred_array.argmax(axis=1))

# Performance matrix training set
Y_Pred=model.predict(X_test)

#converting list to array
Y_Pred_array=np.array(Y_Pred)

#conf matrix
from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(150)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,150])
plt.ylim([0.0,1.5])
plt.savefig('MLPstar_10_CrossEntropy.png')

val_loss.index(min(val_loss[15:35]))

class SparsityRegularizationLayer(layers.Layer):
  C = 0.01
  Tar = 0.20
  def call(self, inputs):
    avAL = tf.reduce_mean(inputs)
    self.add_loss(  self.C * (  -1*self.Tar*tf.math.log(avAL) + (self.Tar-1)*tf.math.log(1-avAL)  )  )
    return inputs

inputs = keras.Input(shape=(400,))
h_star = 607
x1 = layers.Dense(h_star,activation="sigmoid")(inputs)
x1 = SparsityRegularizationLayer()(x1)
outputs = layers.Dense(3, activation="softmax")(x1)
model = keras.Model(inputs=inputs,outputs=outputs)
optimizer = keras.optimizers.Adam()
loss_fn = keras.losses.SparseCategoricalCrossentropy()

model.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['accuracy'])
from tensorflow.keras import callbacks
# monitor `val_accuracy`
checkpoint=callbacks.ModelCheckpoint(filepath='BestModel',monitor='val_accuracy',save_best_only=True)


Monitor = model.fit(X_train,Y_train,epochs=150, batch_size=25,validation_data=(X_test,Y_test),callbacks=[checkpoint])

model.evaluate(X_train,Y_train)

#add confusion matrix code here

# Performance matrix training set
Y_Pred=model.predict(X_train)

#converting list to array
Y_Pred_array=np.array(Y_Pred)

#conf matrix
from sklearn.metrics import confusion_matrix

confusion_matrix(Y_train.argmax(axis=1), Y_Pred_array.argmax(axis=1))

# Performance matrix testing set
Y_Pred=model.predict(X_test)

#converting list to array
Y_Pred_array=np.array(Y_Pred)

#conf matrix
from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(150)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,150])
plt.ylim([0.0,1.5])
plt.savefig('MLPstar_20_CrossEntropy.png')

val_loss.index(min(val_loss[10:25]))

"""**Start: Finding hidden layer activations**"""

np.concatenate((X_train,X_test),axis = 0)

#I think this is activities ??
bestModelhstar10 = tf.keras.models.load_model('BestModel')
whole_data = np.concatenate((X_train,X_test),axis = 0)
arr = bestModelhstar10.layers[1](whole_data).numpy()


hstar_avg_activities = arr.mean(axis=0)
print(hstar_avg_activities)

a = hstar_avg_activities.tolist()
plt.hist(a)
plt.xlabel('Neuron Activity')
plt.ylabel('Frequency')
plt.savefig('NeuronActivity.png')
import statistics 
print(statistics.mean(a))
print(statistics.stdev(a))

"""**Start finding empirical spacities**"""

#Avenrage of hidden layer activity
Avga= statistics.mean(a)
Avga

#Percentage of 1/2
lessthena=0
for i in a:
  if i<Avga/2:
    lessthena +=1
per12=lessthena/len(a)
per12
#0.019769357495881382
#0.008237232289950576

#Percentage of 1/3
lessthena=0
for i in a:
  if i<Avga/3:
    lessthena +=1
per12=lessthena/len(a)
per12
#0.018121911037891267
#0.0016474464579901153



"""#7- implement PCA of  { all vectors Zn }  ;"""

z_test = bestModelhstar10.layers[1](X_test).numpy()
z_train = bestModelhstar10.layers[1](X_train).numpy()

z_train.shape

z_test

#not sure if Zn is the combined z_test and z_train?
Zn = np.concatenate([z_train,z_test])

import sklearn as sl
from sklearn.preprocessing import StandardScaler as ss
from sklearn.decomposition import PCA 
#PCA on Zn
st = ss().fit_transform(Zn)
pca = PCA(0.95)
pc = pca.fit_transform(st) # << to retain the components in an object
#pca.explained_variance_ratio_
print ( "Components = ", pca.n_components_ , ";\nTotal explained variance = ",
      round(pca.explained_variance_ratio_.sum(),5)  )

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.savefig('pcaZn.png')

"""s = 242

**Start:Train auto encoder**
"""

inputs = keras.Input(shape=(607,))

x1 = layers.Dense(272,activation="sigmoid")(inputs)

outputs = layers.Dense(607, activation="sigmoid")(x1)

autoEncoder = keras.Model(inputs=inputs,outputs=outputs)

autoEncoder.compile(optimizer=optimizer,loss=keras.losses.MeanSquaredError())

checkpoint=callbacks.ModelCheckpoint(filepath='BestModel',monitor='val_loss',save_best_only=True)
Monitor = autoEncoder.fit(z_train,z_train,epochs=150, batch_size=25,validation_data=(z_test,z_test),callbacks=[checkpoint])

autoEncoder = load_model('BestModel')

autoEncoder.summary()

"""Need RMSE/average(Zn)"""

val_loss=Monitor.history['val_loss']
MSE = statistics.mean(val_loss)

import math
RMSE = math.sqrt(MSE)

avg_Zn = np.mean(Zn)
print(RMSE)
print(RMSE/avg_Zn)

"""**Attempting q9**"""

Y_pred_train=autoEncoder.predict(z_train)
Y_pred_test=autoEncoder.predict(z_test)

print(Y_pred_train.shape)
print(Y_pred_test.shape)

predlabel_train=np.argmax(Y_pred_train, axis=1)
predlabel_test=np.argmax(Y_pred_test,axis=1)
predlabel_test

inputs=keras.Input(shape=(400,))

x1=layers.Dense(607,activation='sigmoid')(inputs)
x1=SparsityRegularizationLayer()(x1)

outputs=layers.Dense(242,activation= "sigmoid")(x1)

model_new=keras.Model(inputs=inputs,outputs=outputs)

model_new.summary()

model_new.compile(optimizer=optimizer, loss=keras.losses.SparseCategoricalCrossentropy(),metrics=["accuracy"])

checkpoint=callbacks.ModelCheckpoint(filepath="Bestmodel_Kn",monitor='val_loss',save_best_only=True)
monitor=model_new.fit(X_train,Y_train.argmax(axis=1),epochs=150,batch_size=25,validation_data=(X_test,Y_test.argmax(axis=1)),callbacks=[checkpoint])



kn_train=model_new.predict(X_train)
kn_test=model_new.predict(X_test)

Kn=pd.concat([pd.DataFrame(kn_test),
              pd.DataFrame(kn_train)])
np.array(Kn)

Kn.shape

inputs=keras.Input(shape=(242,))

outputs=layers.Dense(3,activation='softmax')(inputs)

model1_new1=keras.Model(inputs=inputs,outputs=outputs)

model1_new1.summary()

model1_new1.compile(optimizer=optimizer,loss=losses.CategoricalCrossentropy(),metrics=["accuracy"])

Monitor=model1_new1.fit(kn_train,Y_train,epochs=150,batch_size=25,validation_data=(kn_test,Y_test))

